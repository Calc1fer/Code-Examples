{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ETCJXdaKtV86"
      },
      "outputs": [],
      "source": [
        "import dlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PItRnlLubS-"
      },
      "outputs": [],
      "source": [
        "# get the pre-trained face detection model (need to run only once)\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 # DOWNLOAD LINK\n",
        "!bunzip2 /content/shape_predictor_68_face_landmarks.dat.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7fYvcUTeuwrS"
      },
      "outputs": [],
      "source": [
        "# prep face detector objects\n",
        "datFile = \"/content/shape_predictor_68_face_landmarks.dat\"\n",
        "face_detector = dlib.get_frontal_face_detector() # locates faces\n",
        "lm_extractor = dlib.shape_predictor(datFile) # computes landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCjug_LHwruO"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # for Colab as imshow crashes there\n",
        "#########################################################\n",
        " # function to draw landamrks\n",
        "def annotate_img(sample, landmarks):\n",
        "\n",
        "    landmark_list = []\n",
        "    for i in range(68):  # Assuming there are 68 landmarks\n",
        "        landmark = landmarks.part(i)\n",
        "        x, y = landmark.x, landmark.y\n",
        "        landmark_list.append((x, y))\n",
        "        colour = (0, 255, 0) # Default colour\n",
        "        if i in [21, 22, 39, 42, 33, 51, 57, 48, 54]:\n",
        "          colour = (0, 0, 255) #Change colour for specific features\n",
        "          cv2.circle(sample, (x,y), 5, colour, -1)\n",
        "        else:\n",
        "          cv2.circle(sample, (x, y), 5, colour, -1)\n",
        "  \n",
        "    return sample\n",
        "\n",
        "#########################################################\n",
        "file_path = '/content/S055_001_01064616.png'\n",
        "label = 'surprise' # should change according to emotion in image\n",
        "sample = cv2.imread(file_path) \n",
        "resized_img = cv2.resize(sample, (720, 720))\n",
        "\n",
        "# any pre-processing (optional)\n",
        "\n",
        "# locate face and then landmarks for each face\n",
        "faces = face_detector(sample) # return a list of faces rectangles if any\n",
        "for face in faces:\n",
        "  f_vector = []\n",
        "  landmarks = lm_extractor(sample, face) # returns object of 68 tuple points (x,y)  \n",
        "\n",
        "  for landmark in landmarks.parts():\n",
        "    x, y = landmark.x, landmark.y\n",
        "    f_vector.append(x)\n",
        "    f_vector.append(y)\n",
        "  \n",
        "  f_vector = [label] + f_vector # append emotion label\n",
        "  print(f_vector)  \n",
        "#########################################################\n",
        "  annotated_sample = annotate_img(sample, landmarks) # to draw landmarks\n",
        "  cv2_imshow(annotated_sample) # comment out when looping over dataset!\n",
        "#########################################################\n",
        "\n",
        "f_vector.append(label)\n",
        "with open('output.csv', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(f_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vxkIIE2fljm"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "#specify the path\n",
        "zip_file_path = '/content/Testing.zip'\n",
        "\n",
        "#Extract the ZIP\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/FacialData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht3sWNotuxXz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "\n",
        "from google.colab.patches import cv2_imshow # for Colab as imshow crashes there\n",
        "\n",
        "#########################################################\n",
        " # function to draw landmarks\n",
        "def annotate_img(sample, landmarks):\n",
        "\n",
        "    landmark_list = []\n",
        "    for i in range(68):  # Assuming there are 68 landmarks\n",
        "        landmark = landmarks.part(i)\n",
        "        x, y = landmark.x, landmark.y\n",
        "        landmark_list.append((x, y))\n",
        "        cv2.circle(sample, (x, y), 1, (0, 255, 0), -1)\n",
        "  \n",
        "    return sample\n",
        "\n",
        "#########################################################\n",
        "\n",
        "#Path to the root folder containing the cartoon character folders\n",
        "root_folder = '/content/FacialData/Training'\n",
        "\n",
        "#List of emotions to label each image\n",
        "emotions = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
        "#Iterate over each of the character folders\n",
        "for character_folder in os.listdir(root_folder):\n",
        "  character_path = os.path.join(root_folder, character_folder)\n",
        "\n",
        "  #Iterate over each folder pertaining to an emotion\n",
        "  for emotion in emotions:\n",
        "    emotion_path = os.path.join(character_path, emotion)\n",
        "\n",
        "    #Iterate over each image in the emotion folder\n",
        "    for image_file in os.listdir(emotion_path):\n",
        "      image_path = os.path.join(emotion_path, image_file)\n",
        "\n",
        "      #Process the image here and display (Not for large datasets)\n",
        "      sample = cv2.imread(image_path)\n",
        "      #resize the image here if need be using the cv2.resize() function\n",
        "\n",
        "      #We can do further processing here such as landmark extraction and annotations.\n",
        "\n",
        "      # locate face and then landmarks for each face\n",
        "      faces = face_detector(sample) # return a list of faces rectangles if any\n",
        "      for face in faces:\n",
        "        f_vector = []\n",
        "        landmarks = lm_extractor(sample, face) # returns object of 68 tuple points (x,y)  \n",
        "\n",
        "        for landmark in landmarks.parts():\n",
        "          x, y = landmark.x, landmark.y\n",
        "          f_vector.append(x)\n",
        "          f_vector.append(y)\n",
        "        \n",
        "        #f_vector = [emotion] + f_vector # append emotion label\n",
        "        #print(f_vector)  \n",
        "      #########################################################\n",
        "        #annotated_sample = annotate_img(sample, landmarks) # to draw landmarks\n",
        "        #cv2_imshow(annotated_sample) # comment out when looping over dataset!\n",
        "      #########################################################\n",
        "\n",
        "        #Save results to a .csv file\n",
        "        f_vector = [emotion] + f_vector #append the emotion label\n",
        "        #f_vector.append(emotion)\n",
        "\n",
        "      csv_path = '/content/all_annotations.csv'\n",
        "\n",
        "      with open(csv_path, 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(f_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "A9Kd8ejCreNA"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "pQcbCHC3ppMi"
      },
      "outputs": [],
      "source": [
        "#Code snipped to make a .csv file with the necessary 'emotion' header\n",
        "\n",
        "data = pd.read_csv('extra_annotations_features.csv', header=None)\n",
        "data.rename(columns={0: 'emotion'}, inplace = True)\n",
        "data.to_csv('extra_annotations_features_label.csv', index=False) #save new files with header\n",
        "\n",
        "#num_columns = len(data.columns)\n",
        "\n",
        "#column_names = ['emotion'] + [f'feature({i})' for i in range(1, num_columns)]\n",
        "\n",
        "#data.columns = column_names\n",
        "\n",
        "#data.to_csv('data_with_column_name.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHGjDCRBUtB6"
      },
      "outputs": [],
      "source": [
        "#TEST THE SVM MODEL ON NEW DATA - VALIDATION STAGE/TESTING DATA\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "\n",
        "def preprocess_and_extract_features(new_image):\n",
        "\n",
        "#Perform preprocessing steps here (normalise/resizing)\n",
        "\n",
        "#Extract the landmark features from the image using the provided landmarks\n",
        "\n",
        "    # locate face and then landmarks for each face\n",
        "  faces = face_detector(new_image) # return a list of faces rectangles if any\n",
        "  landmark_features = [] #List to store landmark features for all faces\n",
        "\n",
        "  for face in faces:\n",
        "    f_vector = []\n",
        "    landmarks = lm_extractor(new_image, face) # returns object of 68 tuple points (x,y)  \n",
        "\n",
        "    for landmark in landmarks.parts():\n",
        "      x, y = landmark.x, landmark.y\n",
        "      f_vector.append(x)\n",
        "      f_vector.append(y)\n",
        "\n",
        "    landmark_features.append(f_vector)\n",
        "    \n",
        "  return landmark_features\n",
        "\n",
        "########################################################\n",
        "#Read .csv to a dataframe\n",
        "\n",
        "#data = pd.read_csv('all_annotations_new.csv')\n",
        "########################################################\n",
        "#Separate the features from the labels\n",
        "y_str = data['emotion']\n",
        "x = data.drop('emotion', axis=1)\n",
        "########################################################\n",
        "# encode the labels, useful when the model cannot process categorical data\n",
        "y = preprocessing.LabelEncoder().fit_transform(list(y_str)) # labels as numbers\n",
        "########################################################\n",
        "#Split data and train a model\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "#Scaling the input data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Fit the scaler on the training data\n",
        "scaler.fit(x_train)\n",
        "\n",
        "#Scale the training and testing data\n",
        "x_train_scaled = scaler.transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# create and train a linear model\n",
        "clf = SVC(kernel='linear', C=1)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_pred = clf.predict(x_test)\n",
        "#print(\"accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "########################################################\n",
        "#Test on new inputs\n",
        "# load a new image\n",
        "\n",
        "#List to store the image paths\n",
        "image_paths = []\n",
        "root_folder = \"/content/drive/MyDrive/FacialData/Validation\"\n",
        "\n",
        "#List of emotions to label each image\n",
        "emotions = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
        "\n",
        "#Iterate through each character folder\n",
        "for character_folder in os.listdir(root_folder):\n",
        "  character_path = os.path.join(root_folder, character_folder)\n",
        "\n",
        "  #Iterate over each folder pertaining to an emotion\n",
        "  for emotion in emotions:\n",
        "    emotion_path = os.path.join(character_path, emotion)\n",
        "\n",
        "    #Iterate over each image in the emotion folder\n",
        "    for image_file in os.listdir(emotion_path):\n",
        "      image_path = os.path.join(emotion_path, image_file)\n",
        "      image_paths.append(image_path)\n",
        "\n",
        "#Select a random subset of image paths\n",
        "num_images = min(len(image_paths), 3790) # number of paths to select\n",
        "random_image_path = random.sample(image_paths, num_images)\n",
        "\n",
        "#iterate over the randomly selected image paths\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "no_predictions = 0\n",
        "\n",
        "for image_path in random_image_path:\n",
        "  new_image = cv2.imread(image_path)\n",
        "  \n",
        "  if new_image is not None:\n",
        "\n",
        "    # extract its features\n",
        "    # predict on the extracted features\n",
        "    new_features = preprocess_and_extract_features(new_image)\n",
        "\n",
        "    #Check if features were found and extracted\n",
        "    if len(new_features) == 0:\n",
        "      #print(\"No features found in this image: \", image_path)\n",
        "      no_predictions += 1\n",
        "      continue\n",
        "\n",
        "    new_features = np.reshape(new_features, (1, -1))\n",
        "    new_prediction = clf.predict(new_features)\n",
        "\n",
        "    #Extract the emotion label from the image file path\n",
        "    emotion = os.path.basename(os.path.dirname(image_path))\n",
        "\n",
        "    #Check if the predicted label matches the ground truth label\n",
        "    if emotion in emotions and new_prediction[0] == emotions.index(emotion):\n",
        "      correct_predictions += 1\n",
        "\n",
        "    total_predictions += 1\n",
        "\n",
        "#Calculate the accuracy here\n",
        "print(\"Total predictions\", total_predictions)\n",
        "print(\"Correct Predictions\" , correct_predictions)\n",
        "accuracy = (correct_predictions / (total_predictions + no_predictions) * 100)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Images with no found features: \", no_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfk7JBig6z5e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC\n",
        "########################################################\n",
        "# function to plot points\n",
        "def plot_2features(x1, x2, y_str, y):\n",
        "  # for plotting with custom legend labels\n",
        "  unique = np.unique(y_str)\n",
        "  u_code = np.unique(y)\n",
        "  colors = [plt.cm.jet(i/float(len(unique)-1)) for i in range(len(unique))]\n",
        "  for i, u in enumerate(u_code):\n",
        "      xi = x1[y == u]\n",
        "      yi = x2[y == u]\n",
        "      plt.scatter(xi, yi, color=colors[u], label=unique[u])\n",
        "  pl = plt.legend()\n",
        "  plt.title('Scatter of two features')\n",
        "########################################################\n",
        "# function to plot decision plane\n",
        "def visualisation(x1, x2, data, y_str, y):\n",
        "  # to visualise decision boundry (from sklearn tutorial)\n",
        "#https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html  \n",
        "  # create a mesh to plot in\n",
        "  h = 1  # step size in the mesh\n",
        "  x_min, x_max = x1.min() - 1, x1.max() + 1\n",
        "  y_min, y_max = x2.min() - 1, x2.max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                      np.arange(y_min, y_max, h))\n",
        "  # classify the mesh points\n",
        "  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  # Put the result into a color plot\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.2, linewidths=0.5)\n",
        "  plt.contour(xx, yy, Z, colors='k', levels=[0], linewidths=2.0)  # Adjust linewidths parameter\n",
        "  # Plot also the training points\n",
        "  #plt.scatter(x1, x2, c=y, cmap=plt.cm.coolwarm)\n",
        "  plot_2features(x1, x2, y_str, y)\n",
        "  plt.xlabel(data.columns[f1])\n",
        "  plt.ylabel(data.columns[f2])\n",
        "  plt.title('SVC Decision Boundary (2 Features)')\n",
        "\n",
        "  plt.show()\n",
        "########################################################\n",
        "# TODO(1): repeat what you did for the previous code cell\n",
        "# read data into dataframe\n",
        "\n",
        "#data = pd.read_csv('annotations_features_train.csv')\n",
        "data = pd.read_csv('all_annotations_new.csv')\n",
        "########################################################\n",
        "# drop all rows except for two classes\n",
        "# you can change to view features for any two classes\n",
        "data = data[(data.emotion == 'joy')| (data['emotion'] == 'anger')]\n",
        "########################################################\n",
        "# separate the features from the labels\n",
        "y_str = data['emotion']\n",
        "x = data.drop('emotion', axis=1)\n",
        "########################################################\n",
        "y = preprocessing.LabelEncoder().fit_transform(list(y_str)) # labels as numbers\n",
        "########################################################\n",
        "# for visualisation, choose any two features\n",
        "# TODO(2): change which two feature are used and observe graphs\n",
        "f1 = 48\n",
        "f2 = 54\n",
        "x1 = x.iloc[:, f1]\n",
        "x2 = x.iloc[:, f2]\n",
        "x  = np.column_stack((x1, x2))\n",
        "########################################################\n",
        "# TODO(3): change model parameters and observe decision function\n",
        "clf = SVC(kernel='linear', C=10.0)\n",
        "clf.fit(x, y)  \n",
        "########################################################\n",
        "visualisation(x1, x2, data, y_str, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "WuAMtn9w4poy",
        "outputId": "cd21fb5c-94db-4ec9-d63e-184065f6ed2f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Read the data from the CSV file\n",
        "#data = pd.read_csv('annotations_features_train.csv')\n",
        "data = pd.read_csv('all_annotations_new.csv')\n",
        "\n",
        "# Separate the features from the labels\n",
        "y_str = data['emotion']\n",
        "x = data.drop('emotion', axis=1)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y_str)\n",
        "\n",
        "# Select two features for plotting the decision boundary\n",
        "x_subset = x.iloc[:, [4, 1]].copy()\n",
        "\n",
        "# Fit the model and get the separating hyperplane\n",
        "clf = svm.SVC(kernel=\"linear\", C=1.0)\n",
        "clf.fit(x_subset, y)\n",
        "\n",
        "# Plot the samples\n",
        "scatter = plt.scatter(x_subset.iloc[:, 0], x_subset.iloc[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
        "\n",
        "# Plot the decision boundary\n",
        "ax = plt.gca()\n",
        "disp = DecisionBoundaryDisplay.from_estimator(\n",
        "    clf,\n",
        "    x_subset.values,\n",
        "    plot_method=\"contour\",\n",
        "    colors=\"k\",\n",
        "    levels=[0],\n",
        "    alpha=0.5,\n",
        "    linestyles=[\"-\"],\n",
        "    ax=ax,\n",
        ")\n",
        "\n",
        "# Get the unique emotions and their corresponding labels\n",
        "unique_emotions = label_encoder.inverse_transform(np.unique(y))\n",
        "handles = [plt.Line2D([], [], marker=\"o\", linestyle=\"\", color=scatter.cmap(scatter.norm(label_encoder.transform([emotion]))[0])) for emotion in unique_emotions]\n",
        "\n",
        "# Add the legend\n",
        "plt.legend(handles, unique_emotions, loc=\"upper right\")\n",
        "plt.xlabel('LeftEyebrow')\n",
        "plt.ylabel('RightEyebrow')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBbfB7Q3LtWZ"
      },
      "outputs": [],
      "source": [
        "#CODE CELL THAT WILL WRITE THE CSV FILE USING THE TRAINING DATASET\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "import csv\n",
        "\n",
        "#########################################################\n",
        "def dist(p1, p2): # Euclidean distance \n",
        "    x1 = p1.x\n",
        "    y1 = p1.y\n",
        "    x2 = p2.x\n",
        "    y2 = p2.y  \n",
        "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "\n",
        "#########################################################\n",
        "# Function to draw landmarks in different colours so we can see what we are targetting\n",
        "def annotate_img(sample, landmarks):\n",
        "    landmark_list = []\n",
        "    for i in range(68):\n",
        "        landmark = landmarks.part(i)\n",
        "        x, y = landmark.x, landmark.y\n",
        "        landmark_list.append((x, y))\n",
        "        colour = (0, 255, 0)  # Default color\n",
        "        if i in [18, 19, 20, 21, 39, 22, 23, 24, 25, 42, 33, 48, 49, 50, 51, 52, 53, 54, 36, 37, 38, 49, 41, 43, 44, 45, 46, 47]:\n",
        "            colour = (0, 0, 255)  # Change colour for specific features\n",
        "        cv2.circle(sample, (x, y), 3, colour, -1)\n",
        "    return sample\n",
        "\n",
        "#########################################################\n",
        "# Function to extract the RIGHT EYE feature\n",
        "def extract_righteye_feature(landmarks):\n",
        "    right_eye_points = [43, 44, 45, 46, 47]  # Right eye landmarks excluding point 39\n",
        "    right_inner_eye_point = 42  # Point to which distances are normalized\n",
        "\n",
        "    # Calculate the sum of normalized distances between left eye landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in right_eye_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(right_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_42_43 = dist(landmarks.part(42), landmarks.part(43))\n",
        "        normalised_distance = distance / distance_42_43\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "# Function to extract the LEFT EYE feature\n",
        "def extract_lefteye_feature(landmarks):\n",
        "    left_eye_points = [36, 37, 38, 49, 41]  # Left eye landmarks excluding point 39\n",
        "    left_inner_eye_point = 39  # Point to which distances are normalized\n",
        "\n",
        "    # Calculate the sum of normalized distances between left eye landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in left_eye_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(left_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_39_38 = dist(landmarks.part(39), landmarks.part(38))\n",
        "        normalised_distance = distance / distance_39_38\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "# Function to extract the LEFT EYEBROW feature\n",
        "def extract_left_eyebrow_feature(landmarks):\n",
        "    left_eyebrow_points = [18, 19, 20, 21]  # Left eyebrow landmarks excluding point 39\n",
        "    left_inner_eye_point = 39  # Point to which distances are normalized\n",
        "\n",
        "    # Calculate the sum of normalized distances between left eyebrow landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in left_eyebrow_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(left_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_39_21 = dist(landmarks.part(39), landmarks.part(21))\n",
        "        normalised_distance = distance / distance_39_21\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function to extract the RIGHT EYEBROW\n",
        "def extract_right_eyebrow_feature(landmarks):\n",
        "    right_eyebrow_points = [22, 23, 24, 25]  # right eyebrow landmarks excluding point 39\n",
        "    right_inner_eye_point = 42  # Point to which distances are normalised\n",
        "\n",
        "    # Calculate the sum of normalised distances between right eyebrow landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in right_eyebrow_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(right_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_42_25 = dist(landmarks.part(42), landmarks.part(25))\n",
        "        normalised_distance = distance / distance_42_25\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function to extract LEFT LIP\n",
        "def extract_left_lip_feature(landmarks):  \n",
        "    left_lip_points = [48, 49, 50, 51]  #Left lip landmarks excluding point 34\n",
        "    left_stationary_point = 33    #Point to which distances are normalised\n",
        "\n",
        "          # Calculate the sum of normalised distances between left lip landmarks and the stationary lip point\n",
        "    dist_sum = 0\n",
        "    for point in left_lip_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(left_stationary_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_33_51 = dist(landmarks.part(33), landmarks.part(51))\n",
        "        normalised_distance = distance / distance_33_51\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function to extract RIGHT_LIP\n",
        "def extract_right_lip_feature(landmarks):\n",
        "    right_lip_points = [52, 53, 54, 51]  #Right lip landmarks excluding point 34\n",
        "    right_stationary_point = 33    #Point to which distances are normalised\n",
        "\n",
        "          # Calculate the sum of normalised distances between right lip landmarks and stationary lip point\n",
        "    dist_sum = 0\n",
        "    for point in right_lip_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(right_stationary_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_33_51 = dist(landmarks.part(33), landmarks.part(51))\n",
        "        normalised_distance = distance / distance_33_51\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum  \n",
        "\n",
        "#########################################################\n",
        "#Function for the LIP WIDTH\n",
        "#Get the distance between edges of lips and normalise\n",
        "def extract_lip_width_feature(landmarks):\n",
        "\n",
        "    dist_sum = 0\n",
        "    distance_width = dist(landmarks.part(48), landmarks.part(54))\n",
        "    distance_normal_points = dist(landmarks.part(33), landmarks.part(51))\n",
        "    normalised_distance = distance_width / distance_normal_points\n",
        "    dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function for the LIP HEIGHT\n",
        "#Distance between the upper and lower lip and normalise\n",
        "def extract_lip_height_feature(landmarks):\n",
        "\n",
        "    dist_sum = 0\n",
        "    distance_width = dist(landmarks.part(51), landmarks.part(57))\n",
        "    distance_normal_points = dist(landmarks.part(33), landmarks.part(51))\n",
        "    normalised_distance = distance_width / distance_normal_points\n",
        "    dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "#########################################################\n",
        "# Function to compute distances and write to CSV\n",
        "def compute_distances_and_write_csv(file_path):\n",
        "    label = os.path.basename(os.path.dirname(file_path))  # Change the label according to the emotion in the image\n",
        "    sample = cv2.imread(file_path)\n",
        "    resized_img = cv2.resize(sample, (720, 720))\n",
        "\n",
        "    # Locate face and then landmarks for each face\n",
        "    faces = face_detector(sample)  # Return a list of face rectangles if any\n",
        "\n",
        "    for face in faces:\n",
        "        f_vector = []\n",
        "        landmarks = lm_extractor(sample, face)  # Returns object of 68 tuple points (x, y)\n",
        "\n",
        "        # Extract left eyebrow feature\n",
        "        left_eye_feature = extract_lefteye_feature(landmarks)\n",
        "        right_eye_feature = extract_righteye_feature(landmarks)\n",
        "        left_eyebrow_feature = extract_left_eyebrow_feature(landmarks)\n",
        "        right_eyebrow_feature = extract_right_eyebrow_feature(landmarks)\n",
        "        left_lip_feature = extract_left_lip_feature(landmarks)\n",
        "        right_lip_feature = extract_right_lip_feature(landmarks)\n",
        "        lip_width_feature = extract_lip_width_feature(landmarks)\n",
        "        lip_height_feature = extract_lip_height_feature(landmarks)\n",
        "\n",
        "        #Append the emotion label and then the features\n",
        "        f_vector.append(label)\n",
        "        f_vector.append(left_eye_feature)\n",
        "        f_vector.append(right_eye_feature)\n",
        "        f_vector.append(left_eyebrow_feature)\n",
        "        f_vector.append(right_eyebrow_feature)\n",
        "        f_vector.append(left_lip_feature)\n",
        "        f_vector.append(right_lip_feature)\n",
        "        f_vector.append(lip_width_feature)\n",
        "        f_vector.append(lip_height_feature)\n",
        "\n",
        "        # Print the feature vector\n",
        "        print(f_vector)\n",
        "\n",
        "        # Annotate the sample image with landmarks\n",
        "        annotated_sample = annotate_img(sample, landmarks)\n",
        "        #cv2_imshow(annotated_sample)  # Comment out when looping over the dataset!\n",
        "\n",
        "        # Write the feature vector to CSV\n",
        "        with open('extra_annotations_features.csv', 'a') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(f_vector)\n",
        "################################################################\n",
        "#List to store the image paths\n",
        "image_paths = []\n",
        "root_folder = \"/content/drive/MyDrive/FacialData/Training\"\n",
        "\n",
        "#List of emotions to label each image\n",
        "emotions = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
        "\n",
        "#Iterate through each character folder\n",
        "for character_folder in os.listdir(root_folder):\n",
        "  character_path = os.path.join(root_folder, character_folder)\n",
        "\n",
        "  #Iterate over each folder pertaining to an emotion\n",
        "  for emotion in emotions:\n",
        "    emotion_path = os.path.join(character_path, emotion)\n",
        "\n",
        "    #Iterate over each image in the emotion folder\n",
        "    for image_file in os.listdir(emotion_path):\n",
        "      image_path = os.path.join(emotion_path, image_file)\n",
        "      image_paths.append(image_path)\n",
        "\n",
        "\n",
        "      # Call the function with the file path of your test image\n",
        "      compute_distances_and_write_csv(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXg_flHUawhK"
      },
      "outputs": [],
      "source": [
        "#CODE CELL TO TEST THE MODEL ON THE VALIDATION DATA USING NEW CSV\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "import csv\n",
        "\n",
        "#########################################################\n",
        "def dist(p1, p2): # Euclidean distance \n",
        "    x1 = p1.x\n",
        "    y1 = p1.y\n",
        "    x2 = p2.x\n",
        "    y2 = p2.y  \n",
        "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "\n",
        "#########################################################\n",
        "# Function to draw landmarks in different colours so we can see what we are targetting\n",
        "def annotate_img(sample, landmarks):\n",
        "    landmark_list = []\n",
        "    for i in range(68):\n",
        "        landmark = landmarks.part(i)\n",
        "        x, y = landmark.x, landmark.y\n",
        "        landmark_list.append((x, y))\n",
        "        colour = (0, 255, 0)  # Default color\n",
        "        if i in [18, 19, 20, 21, 39, 22, 23, 24, 25, 42, 33, 48, 49, 50, 51, 52, 53, 54, 36, 37, 38, 49, 41, 43, 44, 45, 46, 47]:\n",
        "            colour = (0, 0, 255)  # Change colour for specific features\n",
        "        cv2.circle(sample, (x, y), 3, colour, -1)\n",
        "    return sample\n",
        "\n",
        "#########################################################\n",
        "# Function to extract the RIGHT EYE feature\n",
        "def extract_righteye_feature(landmarks):\n",
        "    right_eye_points = [43, 44, 45, 46, 47]  # Right eye landmarks excluding point 39\n",
        "    right_inner_eye_point = 42  # Point to which distances are normalized\n",
        "\n",
        "    # Calculate the sum of normalized distances between left eye landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in right_eye_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(right_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_42_43 = dist(landmarks.part(42), landmarks.part(43))\n",
        "        normalised_distance = distance / distance_42_43\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "# Function to extract the LEFT EYE feature\n",
        "def extract_lefteye_feature(landmarks):\n",
        "    left_eye_points = [36, 37, 38, 49, 41]  # Left eye landmarks excluding point 39\n",
        "    left_inner_eye_point = 39  # Point to which distances are normalized\n",
        "\n",
        "    # Calculate the sum of normalized distances between left eye landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in left_eye_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(left_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_39_38 = dist(landmarks.part(39), landmarks.part(38))\n",
        "        normalised_distance = distance / distance_39_38\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "# Function to extract the LEFT EYEBROW feature\n",
        "def extract_left_eyebrow_feature(landmarks):\n",
        "    left_eyebrow_points = [18, 19, 20, 21]  # Left eyebrow landmarks excluding point 40\n",
        "    left_inner_eye_point = 39  # Point to which distances are normalized\n",
        "\n",
        "    # Calculate the sum of normalized distances between left eyebrow landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in left_eyebrow_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(left_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_39_21 = dist(landmarks.part(39), landmarks.part(21))\n",
        "        normalised_distance = distance / distance_39_21\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function to extract the RIGHT EYEBROW\n",
        "def extract_right_eyebrow_feature(landmarks):\n",
        "    right_eyebrow_points = [22, 23, 24, 25]  # right eyebrow landmarks excluding point 40\n",
        "    right_inner_eye_point = 42  # Point to which distances are normalised\n",
        "\n",
        "    # Calculate the sum of normalised distances between right eyebrow landmarks and inner eye point\n",
        "    dist_sum = 0\n",
        "    for point in right_eyebrow_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(right_inner_eye_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_42_25 = dist(landmarks.part(42), landmarks.part(25))\n",
        "        normalised_distance = distance / distance_42_25\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function to extract LEFT LIP\n",
        "def extract_left_lip_feature(landmarks):  \n",
        "    left_lip_points = [48, 49, 50, 51]  #Left lip landmarks excluding point 34\n",
        "    left_stationary_point = 33    #Point to which distances are normalised\n",
        "\n",
        "          # Calculate the sum of normalised distances between left lip landmarks and the stationary lip point\n",
        "    dist_sum = 0\n",
        "    for point in left_lip_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(left_stationary_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_33_51 = dist(landmarks.part(33), landmarks.part(51))\n",
        "        normalised_distance = distance / distance_33_51\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function to extract RIGHT_LIP\n",
        "def extract_right_lip_feature(landmarks):\n",
        "    right_lip_points = [52, 53, 54, 51]  #Right lip landmarks excluding point 34\n",
        "    right_stationary_point = 33    #Point to which distances are normalised\n",
        "\n",
        "          # Calculate the sum of normalised distances between right lip landmarks and stationary lip point\n",
        "    dist_sum = 0\n",
        "    for point in right_lip_points:\n",
        "        p1 = landmarks.part(point)\n",
        "        p2 = landmarks.part(right_stationary_point)\n",
        "        distance = dist(p1, p2)\n",
        "        distance_33_51 = dist(landmarks.part(33), landmarks.part(51))\n",
        "        normalised_distance = distance / distance_33_51\n",
        "        dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum  \n",
        "\n",
        "#########################################################\n",
        "#Function for the LIP WIDTH\n",
        "#Get the distance between edges of lips and normalise\n",
        "def extract_lip_width_feature(landmarks):\n",
        "\n",
        "    dist_sum = 0\n",
        "    distance_width = dist(landmarks.part(48), landmarks.part(54))\n",
        "    distance_normal_points = dist(landmarks.part(33), landmarks.part(51))\n",
        "    normalised_distance = distance_width / distance_normal_points\n",
        "    dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "\n",
        "#########################################################\n",
        "#Function for the LIP HEIGHT\n",
        "#Distance between the upper and lower lip and normalise\n",
        "def extract_lip_height_feature(landmarks):\n",
        "\n",
        "    dist_sum = 0\n",
        "    distance_width = dist(landmarks.part(51), landmarks.part(57))\n",
        "    distance_normal_points = dist(landmarks.part(33), landmarks.part(51))\n",
        "    normalised_distance = distance_width / distance_normal_points\n",
        "    dist_sum += normalised_distance\n",
        "\n",
        "    return dist_sum\n",
        "#########################################################\n",
        "\n",
        "def preprocess_and_extract_features(new_image):\n",
        "#Perform preprocessing steps here (normalise/resizing)\n",
        "\n",
        "#Extract the landmark features from the image using the provided landmarks\n",
        "   # Locate face and then landmarks for each face\n",
        "    faces = face_detector(new_image)  # Return a list of face rectangles if any\n",
        "\n",
        "    for face in faces:\n",
        "        landmark_features = []\n",
        "        landmarks = lm_extractor(new_image, face)  # Returns object of 68 tuple points (x, y)\n",
        "\n",
        "        # Extract left eyebrow feature\n",
        "        left_eye_feature = extract_lefteye_feature(landmarks)\n",
        "        right_eye_feature = extract_righteye_feature(landmarks)\n",
        "        left_eyebrow_feature = extract_left_eyebrow_feature(landmarks)\n",
        "        right_eyebrow_feature = extract_right_eyebrow_feature(landmarks)\n",
        "        left_lip_feature = extract_left_lip_feature(landmarks)\n",
        "        right_lip_feature = extract_right_lip_feature(landmarks)\n",
        "        lip_width_feature = extract_lip_width_feature(landmarks)\n",
        "        lip_height_feature = extract_lip_height_feature(landmarks)\n",
        "\n",
        "        #Append the emotion label and then the features\n",
        "        landmark_features.append(left_eye_feature)\n",
        "        landmark_features.append(right_eye_feature)\n",
        "        landmark_features.append(left_eyebrow_feature)\n",
        "        landmark_features.append(right_eyebrow_feature)\n",
        "        landmark_features.append(left_lip_feature)\n",
        "        landmark_features.append(right_lip_feature)\n",
        "        landmark_features.append(lip_width_feature)\n",
        "        landmark_features.append(lip_height_feature)\n",
        "\n",
        "        # Print the feature vector\n",
        "        #print(landmark_features)\n",
        "\n",
        "        return landmark_features\n",
        "\n",
        "########################################################\n",
        "#Read .csv to a dataframe\n",
        "\n",
        "data = pd.read_csv('extra_annotations_features_label.csv')\n",
        "########################################################\n",
        "#Separate the features from the labels\n",
        "y_str = data['emotion']\n",
        "x = data.drop('emotion', axis=1)\n",
        "########################################################\n",
        "# encode the labels, useful when the model cannot process categorical data\n",
        "y = preprocessing.LabelEncoder().fit_transform(list(y_str)) # labels as numbers\n",
        "########################################################\n",
        "#Split data and train a model\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "#Scaling the input data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Fit the scaler on the training data\n",
        "scaler.fit(x_train)\n",
        "\n",
        "#Scale the training and testing data\n",
        "x_train_scaled = scaler.transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# create and train a linear model\n",
        "clf = SVC(kernel='linear', C=10)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_pred = clf.predict(x_test)\n",
        "#print(\"accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "########################################################\n",
        "#Test on new inputs\n",
        "# load a new image\n",
        "\n",
        "#List to store the image paths\n",
        "image_paths = []\n",
        "root_folder = \"/content/drive/MyDrive/FacialData/Validation\"\n",
        "\n",
        "#List of emotions to label each image\n",
        "emotions = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
        "predicted_emotions = []\n",
        "\n",
        "#Iterate through each character folder\n",
        "for character_folder in os.listdir(root_folder):\n",
        "  character_path = os.path.join(root_folder, character_folder)\n",
        "\n",
        "  #Iterate over each folder pertaining to an emotion\n",
        "  for emotion in emotions:\n",
        "    emotion_path = os.path.join(character_path, emotion)\n",
        "\n",
        "    #Iterate over each image in the emotion folder\n",
        "    for image_file in os.listdir(emotion_path):\n",
        "      image_path = os.path.join(emotion_path, image_file)\n",
        "      image_paths.append(image_path)\n",
        "\n",
        "#Select a random subset of image paths\n",
        "num_images = min(len(image_paths), 3790) # number of paths to select\n",
        "random_image_path = random.sample(image_paths, num_images)\n",
        "\n",
        "#iterate over the randomly selected image paths\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "no_predictions = 0\n",
        "\n",
        "for image_path in random_image_path:\n",
        "  new_image = cv2.imread(image_path)\n",
        "  \n",
        "  if new_image is not None:\n",
        "\n",
        "    # extract its features\n",
        "    # predict on the extracted features\n",
        "    new_features = preprocess_and_extract_features(new_image)\n",
        "\n",
        "\n",
        "    #Check if features were found and extracted\n",
        "    if new_features is None:\n",
        "      no_predictions += 1\n",
        "      continue\n",
        "\n",
        "    if len(new_features) == 0:\n",
        "      #print(\"No features found in this image: \", image_path)\n",
        "      no_predictions += 1\n",
        "      continue\n",
        "\n",
        "    new_features = np.reshape(new_features, (1, -1))\n",
        "    new_prediction = clf.predict(new_features)\n",
        "\n",
        "    predicted_emotion = emotions[new_prediction[0]]\n",
        "    predicted_emotions.append(predicted_emotion)\n",
        "\n",
        "    #Extract the emotion label from the image file path\n",
        "    emotion = os.path.basename(os.path.dirname(image_path))\n",
        "    #print(\"Predicted Emotion: \",)\n",
        "\n",
        "    #Check if the predicted label matches the ground truth label\n",
        "    if emotion in emotions and new_prediction[0] == emotions.index(emotion):\n",
        "      correct_predictions += 1\n",
        "\n",
        "    total_predictions += 1\n",
        "\n",
        "#Calculate the accuracy here\n",
        "print(\"Total predictions\", total_predictions)\n",
        "print(\"Correct Predictions\" , correct_predictions)\n",
        "accuracy = (correct_predictions / (total_predictions + no_predictions) * 100)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Images with no found features: \", no_predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
